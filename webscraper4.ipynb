{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ChromeDriverManager\n",
    "# pip install webdriver-manager\n",
    "\n",
    "# For substring -need to type this into git, won't work here (at least for me)\n",
    "# pip install substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import substring\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys # Useful for sending keys (not necessarily used below) #cyh\n",
    "from selenium.webdriver.common.action_chains import ActionChains # Useful for queuing commands (not necessarily used below) #cyh\n",
    "from webdriver_manager.chrome import ChromeDriverManager # requires pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data frame to write info to\n",
    "paper_df = pd.DataFrame(index=range(10000), columns=['Title', 'Abstract', 'Keyword', 'Keyword2', 'Keyword3', 'Keyword4', 'Keyword5', 'Journal', 'Author', 'Date', 'General', 'Source', \"Citation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the URL topic here!\n",
    "# I filter down to journal articles only. I also put the search term into quotation marks\n",
    "\n",
    "#url= \"https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&fos=political-science\"\n",
    "url= \"https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=Space&sort=relevance&fos=physics\"\n",
    "\n",
    "# List of topics and their search term links\n",
    "# \"International Affairs\" (selected for political science papers only)\n",
    "# https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&fos=political-science\n",
    "# \"Space\" (selected for Physics papers only)\n",
    "# https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=Space&sort=relevance&fos=physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to see if it works\n",
    "#url= \"https://www.semanticscholar.org/paper/The-Logic-of-New-Media-in-International-Affairs-Kluver/a8c68376fe0011ddfd5b10c413de6fe7264fd485\"\n",
    "\n",
    "#response= requests.get(url)\n",
    "#time.sleep(10)\n",
    "#soup = BeautifulSoup(response.text, \"lxml\")\n",
    "#abstract= soup.find_all(class_= \"text-truncator abstract__text text--preline\")\n",
    "#keywords= soup.find_all(class_ =\"entity-columns\")\n",
    "#author= soup.find_all(class_=\"author-list__link author-list__author-name\")\n",
    "#general= soup.find_all(class_=\"fresh-paper-detail-page__header\")\n",
    "\n",
    "\n",
    "#date= soup.find_all(data-selenium-selector=\"paper-year\")\n",
    "#date= soup.find(\"paper-year\")\n",
    "#date= soup.select(data-selenium-selector.paper-year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for win32 chromedriver:77.0.3865.40 in cache\n",
      "Driver found in C:\\Users\\602772\\.wdm\\chromedriver\\77.0.3865.40\\win32/chromedriver.exe\n",
      "i = 1 https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=Space&sort=relevance&fos=physics&page=1\n",
      "Number of papers are 10\n"
     ]
    }
   ],
   "source": [
    "# WEBSCRAPING CELL\n",
    "\n",
    "# Better version if your computer is able to detect the chromedriver in the install folder; mine cannot #cyh\n",
    "#chrome_driver = \"/Applications/chromedriver\"\n",
    "#os.environ[\"webdriver.chrome.driver\"] = chrome_driver\n",
    "#driver = webdriver.Chrome(chrome_driver)\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# This is the beginning URL of all papers on Semantics Scholar\n",
    "url_base = \"https://www.semanticscholar.org/paper/\"\n",
    "\n",
    "paper_counter = 0\n",
    "paper_counter_temp = 0\n",
    "\n",
    "# Loop through X Results Pages\n",
    "max_page_results = 1\n",
    "for i in range(1,max_page_results+1): # add 1 due to starting at index 1\n",
    "    url_page= url + \"&page=\" + str(i)\n",
    "    driver.get(url_page)\n",
    "    print(\"i = \" + str(i) + \" \" + driver.current_url)\n",
    "    time.sleep(2)    \n",
    "\n",
    "    # Get the paper titles\n",
    "    search_results = driver.find_elements_by_class_name(\"search-result-title\") \n",
    "    paper_counter_temp = paper_counter\n",
    "    for search_result_counter in range(0, len(search_results)):\n",
    "        paper_df['Title'][paper_counter_temp] = search_results[search_result_counter].text\n",
    "        paper_counter_temp = paper_counter_temp + 1\n",
    "    \n",
    "    \n",
    "    hyperlinks = driver.find_elements_by_tag_name(\"a\")\n",
    "    \n",
    "    # Get the URLs to the papers\n",
    "    paper_url_list = []\n",
    "    for hyperlink_counter in range(0, len(hyperlinks)):\n",
    "        hyperlink_url = hyperlinks[hyperlink_counter].get_attribute(\"href\")\n",
    "        # Check if this element had a URL\n",
    "        if hyperlink_url is not None:\n",
    "            # Paper urls begin with \"https://www.semanticscholar.org/paper/\" as defined in url_base\n",
    "            if hyperlink_url[0:len(url_base)] == url_base:\n",
    "                paper_url_list.append(hyperlink_url)\n",
    "            \n",
    "    \n",
    "    print(\"Number of papers are \" + str(len(paper_url_list)))\n",
    "    # try next sibling as an option\n",
    "    # better:  try find_all(\"a\") and look for the href\n",
    "    \n",
    "    # Loop through each search result on the search results page\n",
    "    for j in range(0, len(paper_url_list)):\n",
    "        paper_url_list\n",
    "        # \"Click\" on the link by going directly to the URL. Directly going avoids problems associated with clicking, such as\n",
    "        # banners/etc covering the hyperlinks and inconsistent website behavior.\n",
    "        \n",
    "        driver.get(paper_url_list[j])\n",
    "        \n",
    "        # Locate the \"Continue Reading\" button to expand the abstract and click it.  If no such button found, then do nothing.\n",
    "        try:\n",
    "            clickable_more_button = driver.find_element_by_partial_link_text(\"CONTINUE READING\")\n",
    "            clickable_more_button.click()\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "        \n",
    "        response = requests.get(driver.current_url)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    \n",
    "        citations= soup.find_all(class_=\"bibtex-citation\")\n",
    "        #abstracts= soup.find_all(class_= \"text-truncator abstract__text text--preline\")\n",
    "        abstracts= soup.find_all(class_= \"fresh-paper-detail-page__abstract\")\n",
    "        keywords= soup.find_all(class_ =\"preview-box__target\")\n",
    "        generals= soup.find_all(class_=\"schema-data\")\n",
    "        \n",
    "        # Paper Titles are added near the top of the Search Results page loop\n",
    "        if len(abstracts) > 0:\n",
    "            paper_df['Abstract'][paper_counter] = abstracts[0].text\n",
    "        if len(keywords) > 0:\n",
    "            paper_df['Keyword'][paper_counter] = keywords[0].text\n",
    "            if len(keywords) > 1:\n",
    "                paper_df['Keyword2'][paper_counter] = keywords[1].text\n",
    "                if len(keywords) > 2:\n",
    "                    paper_df['Keyword3'][paper_counter] = keywords[2].text\n",
    "                    if len(keywords) > 3:\n",
    "                        paper_df['Keyword4'][paper_counter] = keywords[3].text\n",
    "                        if len(keywords) > 4:\n",
    "                            paper_df['Keyword5'][paper_counter] = keywords[4].text\n",
    "        #This cleans and adds citation and general fields to dataframe\n",
    "        citations= citations[0].text\n",
    "        paper_df['Citation'][paper_counter] = \"\".join(s for s in citations if ord(s)>31 and ord(s)<126)\n",
    "        generals= generals[0].text\n",
    "        paper_df['General'][paper_counter] = \"\".join(s for s in generals if ord(s)>31 and ord(s)<126)\n",
    "\n",
    "#This should clean the citation column to get date, authors, and journal. \n",
    "        citation= paper_df['Citation'][paper_counter]\n",
    "        citation2= citation.split(\",\")\n",
    "\n",
    "        for i in range(0, len(citation2)):\n",
    "            if \"author=\" in citation2[i]:\n",
    "                author= citation2[i]\n",
    "            if \"journal=\" in citation2[i]:\n",
    "                journal= citation2[i]\n",
    "            if \"year=\" in citation2[i]:\n",
    "                date= citation2[i]\n",
    "        \n",
    "        paper_df['Author'][paper_counter] = author\n",
    "        paper_df['Journal'][paper_counter] = journal\n",
    "        paper_df['Date'][paper_counter] = date\n",
    "        \n",
    "        \n",
    "        paper_df['Source'][paper_counter] = driver.current_url\n",
    "        paper_counter = paper_counter + 1\n",
    "        \n",
    "        # Return to Search Results page\n",
    "        driver.back()\n",
    "        if driver.current_url != url_page:\n",
    "            driver.get(url_page)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "# Purposely write the results as they come rather than all at once\n",
    "# paper_df.to_pickle(\"./paper_data.pkl\")\n",
    "\n",
    "paper_df.to_csv(\"Paper_data1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a string, but acts weirdly like a series. Why does only first part before the common show up?\n",
    "# Testing\n",
    "\n",
    "#start= citation.find()\n",
    "#substring.substringByInd(citation,startInd=20,endInd= 300)\n",
    "#substring.substringByChar(citation)\n",
    "#citation2\n",
    "\n",
    "#can sort of fix by splitting on commas\n",
    "#citation2= citation.split(\",\")\n",
    "#citation2[2]\n",
    "\n",
    "#general= paper_df['General'][2]\n",
    "# general2= substring.substringByChar(general, startChar=\"@\")\n",
    "#general2 = general.split(\",\")\n",
    "#general2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING CELL\n",
    "\n",
    "# Read in data (especially if the webscraping cell execution was skipped)\n",
    "# paper_df = pd.read_pickle(\"./paper_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell for testing access to web elements\n",
    "\n",
    "#url_page= url + \"&page=\" + \"2\"\n",
    "#driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#driver.get(url_page)\n",
    "#time.sleep(2)\n",
    "#soupResultsPage = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#search_results = driver.find_elements_by_class_name(\"search-result-title\")\n",
    "#search_results[1].click()\n",
    "#time.sleep(1)\n",
    "\n",
    "#driver.back()\n",
    "#time.sleep(1)\n",
    "#search_results = driver.find_elements_by_class_name(\"search-result-title\")\n",
    "#search_results[2].click()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
