{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command Prompt installs\n",
    "#\n",
    "# For BeautifulSoup\n",
    "# ...\n",
    "#\n",
    "# For ChromeDriverManager\n",
    "# pip install webdriver-manager\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys # Useful for sending keys (not necessarily used below) #cyh\n",
    "from selenium.webdriver.common.action_chains import ActionChains # Useful for queuing commands (not necessarily used below) #cyh\n",
    "from webdriver_manager.chrome import ChromeDriverManager # requires pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will scrape pages from Semantics Scholar\n",
    "# I filter down to journal articles only. I also put the search term into quotation marks.\n",
    "\n",
    "# \"International Affairs\": \n",
    "# https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version using CSV (now uses pkl files)\n",
    "#with open(\"Paper_data1.csv\",\"w\",newline= \"\") as csvfile:\n",
    "#    writer= csv.writer(csvfile)\n",
    "#    writer.writerow([\"Source\", \"Abstract\",\"Keywords\", \"Journal\", \"Authors\", \"Date\", \"General\"])\n",
    "#    #rows =zip(source, abstract_list, keyword_list, author_list, general_list)\n",
    "\n",
    "paper_df = pd.DataFrame(index=range(10000), columns=['Title', 'Abstract', 'Keyword', 'Journal', 'Author', 'Date', 'General', 'Source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the URL here!\n",
    "# I filter down to journal articles only. I also put the search term into quotation marks.\n",
    "\n",
    "url= \"https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easier to open each entry and scrape from it's full page\n",
    "# Need to script Selenium for that\n",
    "#\n",
    "# To open papers click: <a href=...   data-selenium-selector=\"title-link\">\n",
    "#\n",
    "# To expand full abstract need to click <a class=\"text-truncator__toggle mod-clickable more\" role=\"button\" \n",
    "#                                       aria-label=\"Expand truncated text\" data-selenium-selector=\"text-truncator-toggle\">\n",
    "#                                       CONTINUE READING</a>\n",
    "#\n",
    "# Then do that for each paper down the page.\n",
    "# 10 per page\n",
    "# Then can manually change pages (or automate it if can figure out how)\n",
    "# End of url is &page=2, sequential for each new page\n",
    "# Grab first 20 pages, 10 entries per page, per topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to see if it works\n",
    "#url= \"https://www.semanticscholar.org/paper/The-Logic-of-New-Media-in-International-Affairs-Kluver/a8c68376fe0011ddfd5b10c413de6fe7264fd485\"\n",
    "\n",
    "#response= requests.get(url)\n",
    "#time.sleep(10)\n",
    "#soup = BeautifulSoup(response.text, \"lxml\")\n",
    "#abstract= soup.find_all(class_= \"text-truncator abstract__text text--preline\")\n",
    "#keywords= soup.find_all(class_ =\"entity-columns\")\n",
    "#author= soup.find_all(class_=\"author-list__link author-list__author-name\")\n",
    "#general= soup.find_all(class_=\"fresh-paper-detail-page__header\")\n",
    "\n",
    "\n",
    "#date= soup.find_all(data-selenium-selector=\"paper-year\")\n",
    "#date= soup.find(\"paper-year\")\n",
    "#date= soup.select(data-selenium-selector.paper-year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for win32 chromedriver:76.0.3809.126 in cache\n",
      "Driver found in C:\\Users\\548253\\.wdm\\chromedriver\\76.0.3809.126\\win32/chromedriver.exe\n",
      "i = 1 https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "Number of papers are 10\n",
      "j = 0 while 'clicking' on jth search result https://www.semanticscholar.org/paper/International-Affairs-Portal%3A-A-Semantic-Web-Contreras-Benjamins/d4177744d80786b8c0a742b11e59a054811f5bd1\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 0\n",
      "j = 1 while 'clicking' on jth search result https://www.semanticscholar.org/paper/The-Logic-of-New-Media-in-International-Affairs-Kluver/a8c68376fe0011ddfd5b10c413de6fe7264fd485\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 1\n",
      "j = 2 while 'clicking' on jth search result https://www.semanticscholar.org/paper/A-Semantic-Portal-for-the-International-Affairs-Contreras-Benjamins/6bcea9ea5bfc1dbcb946b80f028694d02768f930\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 2\n",
      "j = 3 while 'clicking' on jth search result https://www.semanticscholar.org/paper/International-Heliophysical-Year-Activities-in-Yumoto/bf22a8b8f0af17b4664964b9c8db6842ac6fbb0f\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 3\n",
      "j = 4 while 'clicking' on jth search result https://www.semanticscholar.org/paper/On-the-Teaching-of-Science%2C-Technology-and-Affairs-Weiss/235c86f8f8ae21ae85f50459d1a3cf59f49e6802\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 4\n",
      "j = 5 while 'clicking' on jth search result https://www.semanticscholar.org/paper/Knowledge-Ecologies-in-International-Affairs%3A-A-New-Costigan-Pallaris/30bfe4c84db73722004a675e370d11f70bd91217\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 5\n",
      "j = 6 while 'clicking' on jth search result https://www.semanticscholar.org/paper/Television's-window-on-the-world%3A-International-on-Garay/b3c0fadcf939d0277ef167a78e9c3bcfcce79f3d\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 6\n",
      "j = 7 while 'clicking' on jth search result https://www.semanticscholar.org/paper/U.S.-health-care-spending-in-an-international-Reinhardt-Hussey/6bd63d8461e5dfa72aa31aa0a1c2ea8f1b31d021\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 7\n",
      "j = 8 while 'clicking' on jth search result https://www.semanticscholar.org/paper/Dealing-with-femtorisks-in-international-relations.-Frank-Collins/03b3eefcbf0cb14d30c0b49727acdf0ad2f7c94d\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 8\n",
      "j = 9 while 'clicking' on jth search result https://www.semanticscholar.org/paper/The-end-of-biomilitary-realism-Rethinking-and-Arya/1f5186df2693abb1a5cd2d374c89771d50b60f77\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=1\n",
      "end of j iteration 9\n",
      "end of i iteration 1\n",
      "i = 2 https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "Number of papers are 10\n",
      "j = 0 while 'clicking' on jth search result https://www.semanticscholar.org/paper/The-end-of-biomilitary-realism-Rethinking-and-Arya/1f5186df2693abb1a5cd2d374c89771d50b60f77\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 0\n",
      "j = 1 while 'clicking' on jth search result https://www.semanticscholar.org/paper/Science-diplomacy%3A-Investigating-the-perspective-of-Faehnrich/72acf310e173682f7f324bb9f182fe8294a61b5f\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 1\n",
      "j = 2 while 'clicking' on jth search result https://www.semanticscholar.org/paper/Beyond-health-aid%3A-would-an-international-scheme-Ooms-Hammonds/842cb91dad409193fc73732975c4b5fddf7d365c\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 2\n",
      "j = 3 while 'clicking' on jth search result https://www.semanticscholar.org/paper/Junk-News-on-Military-Affairs-and-National-Social-Gallacher-Barash/b000e2fc36c07d2f7fd616c507aeb7ac44e2d612\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 3\n",
      "j = 4 while 'clicking' on jth search result https://www.semanticscholar.org/paper/International-assistance-and-cooperation-for-access-Mok/eb7db2d70ed74a46dc8d6ea8f5be7ec30796e8f6\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 4\n",
      "j = 5 while 'clicking' on jth search result https://www.semanticscholar.org/paper/Bioterrorism-Related-Anthrax%3A-International-by-the-Polyak-Macy/b4c21ba0566c95e59fe137011ab7c42f8a01c36d\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 5\n",
      "j = 6 while 'clicking' on jth search result https://www.semanticscholar.org/paper/International-Collaboration-in-Science%3A-The-Global-Leydesdorff-Wagner/313c0fc89cd27dfe02ea1ec038414005b6f1f8a8\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 6\n",
      "j = 7 while 'clicking' on jth search result https://www.semanticscholar.org/paper/Walking-and-cycling-to-health%3A-a-comparative-of-and-Pucher-Buehler/7b8046979fa2bea6461be5fc86f7eddb3bf2d252\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 7\n",
      "j = 8 while 'clicking' on jth search result https://www.semanticscholar.org/paper/Regaining-momentum-for-international-climate-policy-Huettner-Freibauer/93a2e710c1e6a69af6a90d5cdfd1ce05617fad3b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 8\n",
      "j = 9 while 'clicking' on jth search result https://www.semanticscholar.org/paper/National-narcissism%3A-Internal-dimensions-and-Cai-Gries/bde3412f77cea9446313831131690b6a683c15a9\n",
      "After returning to Results Page (equivalent of clicking 'back'), url is https://www.semanticscholar.org/search?publicationType%5B0%5D=JournalArticle&q=%22international%20affairs%22&sort=relevance&page=2\n",
      "end of j iteration 9\n",
      "end of i iteration 2\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# WEBSCRAPING CELL\n",
    "#\n",
    "# From each page:\n",
    "# Title will be in:<h1 data-selenium-selector=\"paper-detail-title\"></h1>\n",
    "# class= fresh-paper-detail-page__header\n",
    "# Title, author, journal, year, volume, pages\n",
    "# Abstract will be in:<div class=\"text-truncator abstract__text text--preline\">\n",
    "# Keywords will be in: <div class=\"entity-columns\" data-selenium-selector=\"entities-list\"><div class=\"flex-container flex-wrap\"><div class=\"flex-item\"><ul class=\"entity-column\">\n",
    "#                      <li class=\"entity-group-item\" data-selenium-selector=\"entity-name\" data-heap-id=\"entity_click\"><span class=\"preview-box__target\">\n",
    "#                       <div class=\"flex-item\"><ul class=\"entity-column\">\n",
    "# Journal will be in: <span data-selenium-selector=\"venue-metadata\"><span><span>New Media &amp; Society</span></span></span>\n",
    "# Authors names will be in: <a class=\"author-list__link author-list__author-name\"\n",
    "# Date will be in: <span data-selenium-selector=\"paper-year\">\n",
    "\n",
    "# Better version if your computer is able to detect the chromedriver in the install folder; mine cannot #cyh\n",
    "#chrome_driver = \"/Applications/chromedriver\"\n",
    "#os.environ[\"webdriver.chrome.driver\"] = chrome_driver\n",
    "#driver = webdriver.Chrome(chrome_driver)\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "#source_list = []\n",
    "#abstract_list =[]\n",
    "#keyword_list =[]\n",
    "#author_list = []\n",
    "#general_list= []\n",
    "\n",
    "# This is the beginning URL of all papers on Semantics Scholar\n",
    "url_base = \"https://www.semanticscholar.org/paper/\"\n",
    "\n",
    "paper_counter = 0\n",
    "paper_counter_temp = 0\n",
    "\n",
    "\n",
    "\n",
    "# Loop through X Results Pages\n",
    "max_page_results = 2\n",
    "for i in range(1,max_page_results+1): # add 1 due to starting at index 1\n",
    "    url_page= url + \"&page=\" + str(i)\n",
    "    driver.get(url_page)\n",
    "    print(\"i = \" + str(i) + \" \" + driver.current_url)\n",
    "    time.sleep(2)    \n",
    "    \n",
    "    # Get the paper titles\n",
    "    search_results = driver.find_elements_by_class_name(\"search-result-title\") \n",
    "    paper_counter_temp = paper_counter\n",
    "    for search_result_counter in range(0, len(search_results)):\n",
    "        paper_df['Title'][paper_counter_temp] = search_results[search_result_counter].text\n",
    "        paper_counter_temp = paper_counter_temp + 1\n",
    "    \n",
    "    \n",
    "    hyperlinks = driver.find_elements_by_tag_name(\"a\")\n",
    "    \n",
    "    # Get the URLs to the papers\n",
    "    paper_url_list = []\n",
    "    for hyperlink_counter in range(0, len(hyperlinks)):\n",
    "        hyperlink_url = hyperlinks[hyperlink_counter].get_attribute(\"href\")\n",
    "        # Check if this element had a URL\n",
    "        if hyperlink_url is not None:\n",
    "            # Paper urls begin with \"https://www.semanticscholar.org/paper/\" as defined in url_base\n",
    "            if hyperlink_url[0:len(url_base)] == url_base:\n",
    "                paper_url_list.append(hyperlink_url)\n",
    "            \n",
    "    \n",
    "    print(\"Number of papers are \" + str(len(paper_url_list)))\n",
    "    # try next sibling as an option\n",
    "    # better:  try find_all(\"a\") and look for the href\n",
    "    \n",
    "    # Loop through each search result on the search results page\n",
    "    for j in range(0, len(paper_url_list)):\n",
    "    #for j in range(0, 2):\n",
    "        # Repeated definition of search_results avoids stale element error so it is purposely repeated\n",
    "        #search_results = driver.find_elements_by_class_name(\"search-result-title\") \n",
    "        #print(\"Number of results are \" + str(len(search_results)))\n",
    "        # try\n",
    "        \n",
    "        # Try to click on the next Title.  Sometimes the title is blocked by the banner, so scroll down to make it visible\n",
    "        #try:\n",
    "        #    search_results[j].click()\n",
    "        #except:\n",
    "        #    #actions = ActionChains(driver)\n",
    "        #    #actions.move_to_element(search_results[j]).perform()\n",
    "        #    #time.sleep(1)\n",
    "        #    # Scroll down to the Title of interest\n",
    "        #    driver.execute_script(\"arguments[0].scrollIntoView();\", search_results[j])\n",
    "        #    # Scroll back up slightly as the upper part of the site covers what we scrolled to\n",
    "        #    driver.execute_script(\"window.scrollBy(0, -150)\", \"\")\n",
    "        #    #time.sleep(1)\n",
    "        #    search_results[j].click()\n",
    "        \n",
    "        paper_url_list\n",
    "        # \"Click\" on the link by going directly to the URL. Directly going avoids problems associated with clicking, such as\n",
    "        # banners/etc covering the hyperlinks and inconsistent website behavior.\n",
    "        \n",
    "        driver.get(paper_url_list[j])\n",
    "        print(\"j = \" + str(j) + \" while 'clicking' on jth search result \" + driver.current_url)\n",
    "        response = requests.get(driver.current_url)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        ## Locate the \"Continue Reading\" button to expand the abstract and click it.  If no such button found, then do nothing.\n",
    "        # This functionality is likely unnecessary if the correct Abstract source is found since the abstract in full is\n",
    "        # available in another webelement.  \n",
    "        #try:\n",
    "        #    clickable_more_button = driver.find_element_by_partial_link_text(\"CONTINUE READING\")\n",
    "        #    clickable_more_button.click()\n",
    "        #except:\n",
    "        #    pass\n",
    "        #time.sleep(1)\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        \n",
    "        # Need to get the title\n",
    "        \n",
    "        # #cyh The abstracts location is incorrect as it only pulls the truncated abstract regardless.  Need to go back later\n",
    "        # and find it properly.  See <meta name=\"description\" underneath the <title>.  If my guess is correct, it probably\n",
    "        # wasn't even necessary to click on the \"CONTINUE READING\" button to get the whole abstract.  This means we may be\n",
    "        # able to remove that \"click on CONTINUE READING\" code and save ourselves some wait time\n",
    "        abstracts= soup.find_all(class_= \"text-truncator abstract__text text--preline\")\n",
    "        keywords= soup.find_all(class_ =\"entity-columns\")\n",
    "        authors= soup.find_all(class_=\"author-list__link author-list__author-name\")\n",
    "        generals= soup.find_all(class_=\"fresh-paper-detail-page__header\")\n",
    "    \n",
    "        \n",
    "        #source_list.append(driver.current_url)\n",
    "        #for abstract in abstracts:\n",
    "        #    abstract_list.append(abstract.text)\n",
    "        #for keyword in keywords:\n",
    "        #    keyword_list.append(keyword.text)\n",
    "        #for author in authors:\n",
    "        #    author_list.append(author.text)\n",
    "        #for general in generals:\n",
    "        #    general_list.append(general.text) \n",
    "        \n",
    "        # Paper Titles are added near the top of the Search Results page loop\n",
    "        if len(abstracts) > 0:\n",
    "            paper_df['Abstract'][paper_counter] = abstracts[0].text\n",
    "        if len(keywords) > 0:\n",
    "            paper_df['Keyword'][paper_counter] = keywords[0].text\n",
    "        if len(authors) > 0:\n",
    "            paper_df['Author'][paper_counter] = authors[0].text\n",
    "        if len(generals) > 0:\n",
    "            paper_df['General'][paper_counter] = generals[0].text\n",
    "        paper_df['Source'][paper_counter] = driver.current_url\n",
    "        paper_counter = paper_counter + 1\n",
    "\n",
    "\n",
    "        #source= [url_page]*len(abstract_list)\n",
    "        \n",
    "        # Return to Search Results page\n",
    "        driver.back()\n",
    "        if driver.current_url != url_page:\n",
    "            driver.get(url_page)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        print(\"After returning to Results Page (equivalent of clicking 'back'), url is \" + driver.current_url)\n",
    "        print(\"end of j iteration \" + str(j))\n",
    "    \n",
    "        \n",
    "    \n",
    "    # Purposely write the results as they come rather than all at once\n",
    "    #with open(\"Paper_data1.csv\", \"a\", newline= \"\") as csvfile:\n",
    "    #    wr =csv.writer(csvfile)\n",
    "    #    rows =zip(source_list, abstract_list, keyword_list, author_list, general_list)\n",
    "    #    wr.writerows(rows)\n",
    "        \n",
    "    print(\"end of i iteration \" + str(i))\n",
    "\n",
    "    \n",
    "# Purposely write the results as they come rather than all at once\n",
    "paper_df.to_pickle(\"./paper_data.pkl\")\n",
    "\n",
    "# Preview part of what was saved.\n",
    "paper_df.head(20)\n",
    "\n",
    "print(\"done\")\n",
    "# Use NLPK, Spacey, etc later for nlp\n",
    "# put into dataframe\n",
    "# or changing the encoding of the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING CELL\n",
    "# Continue coding here.\n",
    "\n",
    "# Read in data (especially if the webscraping cell execution was skipped)\n",
    "paper_df = pd.read_pickle(\"./paper_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for win32 chromedriver:76.0.3809.126 in cache\n",
      "Driver found in C:\\Users\\548253\\.wdm\\chromedriver\\76.0.3809.126\\win32/chromedriver.exe\n"
     ]
    }
   ],
   "source": [
    "# This cell for testing access to web elements\n",
    "\n",
    "url_page= url + \"&page=\" + \"2\"\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url_page)\n",
    "time.sleep(2)\n",
    "#soupResultsPage = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "search_results = driver.find_elements_by_class_name(\"search-result-title\")\n",
    "#search_results[1].click()\n",
    "#time.sleep(1)\n",
    "\n",
    "#driver.back()\n",
    "#time.sleep(1)\n",
    "#search_results = driver.find_elements_by_class_name(\"search-result-title\")\n",
    "#search_results[2].click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVERYTHING BELOW THIS CELL IS SAMPLE CODE.  It's not for use in our NLP code except as an example to imitate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_page= url \n",
    "# for a later step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response= requests.get(url_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes=soup.find_all(class_=\"text\")\n",
    "authors= soup.find_all(class_= \"author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just tests if it works.\n",
    "#print(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also tests if it works.\n",
    "# print(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(quotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_list =[]\n",
    "author_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote in quotes:\n",
    "    quote_list.append(quote.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in authors:\n",
    "    author_list.append(author.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source= [url_page]*len(quote_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"quote_info.csv\", \"a\", newline= \"\") as csvfile:\n",
    "#     wr =csv.writer(csvfile)\n",
    "#     rows =zip(source, quote_list, author_list)\n",
    "#     wr.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also tests if it works.\n",
    "# quote_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
